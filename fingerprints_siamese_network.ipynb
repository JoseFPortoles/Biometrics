{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fingerprints_siamese_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseFPortoles/Biometrics/blob/main/fingerprints_siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM0aKkJNe58f"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Development of a siamese network for fingerprint comparison. The dataset used for training and testing the model is FVC2002 (Second International Competition for Fingerprint Verification Algorithms) and it can be found in http://bias.csr.unibo.it/fvc2002/.\n",
        "\n",
        "Databases present in FVC2002 are:\n",
        "\n",
        "* DB1: optical sensor \"TouchView II\" by Identix\n",
        "* DB2: optical sensor \"FX2000\" by Biometrika\n",
        "* DB3: capacitive sensor \"100 SC\" by Precise Biometrics\n",
        "* DB4: synthetic fingerprint generation\n",
        "\n",
        "Each database contains 880 pictures corresponding to 110 fingers (8 pictures of each finger).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eKTyqOYTkop"
      },
      "source": [
        "## Import stuff\n",
        "All the libraries used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTx257-_TlEF"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Flatten, Concatenate, Dense, Dropout, Subtract, Multiply\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import random\n",
        "import tqdm.notebook as tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import table \n",
        "from itertools import product"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am-Y989LtZw-"
      },
      "source": [
        "# Download database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWrbMn08kASP"
      },
      "source": [
        "# Paths\n",
        "url_db1 = 'http://bias.csr.unibo.it/fvc2002/Downloads/DB1_B.zip'\n",
        "dir_db1 = './FVC2002/DB1_B'\n",
        "url_db2 = 'http://bias.csr.unibo.it/fvc2002/Downloads/DB2_B.zip'\n",
        "dir_db2 = './FVC2002/DB2_B'\n",
        "\n",
        "# Define download function\n",
        "def download_DBi_B(url, dir):\n",
        "    ''' Download a DBi_B database from url to dir'''\n",
        "    zip_name = os.path.basename(url)\n",
        "    path_zip = os.path.join(dir, zip_name)\n",
        "    html_db = requests.get(url)\n",
        "\n",
        "    # Create paths if not already existing\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    \n",
        "    # Download zip file\n",
        "    with open(path_zip, 'wb') as r:\n",
        "        r.write(html_db.content)\n",
        "    \n",
        "    # Unzip database\n",
        "    with zipfile.ZipFile(path_zip, 'r') as z:\n",
        "        z.extractall(dir)\n",
        "    \n",
        "    # Remove zip file\n",
        "    os.remove(path_zip)\n",
        "\n",
        "\n",
        "\n",
        "# Download DBs\n",
        "download_DBi_B(url_db1, dir_db1)\n",
        "download_DBi_B(url_db2, dir_db2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLc_CoOj2-S"
      },
      "source": [
        "### SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dDnKcDJjxzC"
      },
      "source": [
        "# Input height and width\n",
        "img_h = 224\n",
        "img_w = 224\n",
        "\n",
        "# Batch size and Epochs\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "\n",
        "# Number of training/validation examples (pairs of images) generated\n",
        "ntrainfiles = 3200\n",
        "nvalfiles = 3200"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Xcr2h3QN0v"
      },
      "source": [
        "## Preprocessing\n",
        "Here, the folder of interest **DB1_B** of the FP database is used as training folder. Another one **DB3_B** (acquired with a different instrument but still optical acquisition) is used as a validation set in order to assess the ability of the network to generalise, and therefore to perform comparisons of FP not in the database of interest.\n",
        "\n",
        "This cell resizes the original FP raw images to 224 x 224 pixels so they can be feed to the network during training without wasting GPU time while the CPU resizes the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y64B1cXdJGz8"
      },
      "source": [
        "def resize_DB(input, output):\n",
        "    '''Resize the images at input and put the resized versions in output'''\n",
        "    # Create output folders if needed\n",
        "    os.makedirs(output, exist_ok=True)    \n",
        "    # Resize input images\n",
        "    for root, _, files in os.walk(input):\n",
        "        for file in files:\n",
        "            path = os.path.join(root, file)\n",
        "            if '.tif' not in file or 'checkpoint' in file:\n",
        "                print(file)\n",
        "                continue\n",
        "            img = cv2.imread(path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            resized_img = cv2.resize(img, (img_h, img_w))\n",
        "            filename = file.split(sep='.')[0] + '.png'\n",
        "            outpath = os.path.join(output, filename)\n",
        "            cv2.imwrite(outpath, resized_img)\n",
        "\n",
        "# input_dir: Original images√± resized_dir: resized\n",
        "input_dir = '/content/FVC2002/DB2_B'\n",
        "resized_dir = '/content/FVC2002/resized/DB2_B'\n",
        "\n",
        "resize_DB(input_dir, resized_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK3qiLnmrOdf",
        "outputId": "5a6c88e4-6054-4151-909e-2c87d0631c57"
      },
      "source": [
        "resized_x = os.listdir(resized_dir)\n",
        "resized_y = []\n",
        "\n",
        "for filename in resized_x:\n",
        "    id = filename[1:3]\n",
        "    id_num = int(id)\n",
        "    resized_y.append(id_num)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(resized_x, resized_y, test_size=0.35, random_state=42, shuffle=True)\n",
        "\n",
        "print('x_train: ',len(x_train), '\\n', x_train )\n",
        "print('x_test: ', len(x_test), '\\n', x_test)\n",
        "print('y_train: ', len(y_train), '\\n', y_train)\n",
        "print('y_test: ', len(y_test), '\\n', y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train:  52 \n",
            " ['105_3.png', '110_4.png', '109_3.png', '104_1.png', '103_6.png', '108_4.png', '110_3.png', '102_2.png', '106_5.png', '106_4.png', '101_7.png', '104_2.png', '109_7.png', '104_4.png', '107_4.png', '109_8.png', '101_6.png', '103_4.png', '101_8.png', '102_5.png', '105_7.png', '108_5.png', '108_6.png', '105_2.png', '110_5.png', '103_7.png', '104_3.png', '107_7.png', '107_8.png', '104_6.png', '108_7.png', '106_8.png', '106_2.png', '105_1.png', '102_1.png', '109_5.png', '108_8.png', '102_3.png', '110_8.png', '109_6.png', '110_2.png', '104_8.png', '103_8.png', '103_5.png', '107_3.png', '105_5.png', '101_3.png', '105_4.png', '106_7.png', '102_4.png', '106_3.png', '105_6.png']\n",
            "x_test:  28 \n",
            " ['107_5.png', '101_2.png', '106_6.png', '109_2.png', '105_8.png', '104_7.png', '110_6.png', '107_1.png', '107_6.png', '103_1.png', '101_1.png', '110_1.png', '103_3.png', '107_2.png', '108_1.png', '110_7.png', '109_4.png', '102_6.png', '109_1.png', '108_3.png', '106_1.png', '102_8.png', '102_7.png', '104_5.png', '103_2.png', '101_5.png', '101_4.png', '108_2.png']\n",
            "y_train:  52 \n",
            " [5, 10, 9, 4, 3, 8, 10, 2, 6, 6, 1, 4, 9, 4, 7, 9, 1, 3, 1, 2, 5, 8, 8, 5, 10, 3, 4, 7, 7, 4, 8, 6, 6, 5, 2, 9, 8, 2, 10, 9, 10, 4, 3, 3, 7, 5, 1, 5, 6, 2, 6, 5]\n",
            "y_test:  28 \n",
            " [7, 1, 6, 9, 5, 4, 10, 7, 7, 3, 1, 10, 3, 7, 8, 10, 9, 2, 9, 8, 6, 2, 2, 4, 3, 1, 1, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5QDLkGqWcv8"
      },
      "source": [
        "## Data generation\n",
        "Here the iterators which generate the batches for training and validation are defined.\n",
        "\n",
        "It starts by loading the images of the relevant database to a numpy array. The network works by comparing a pair of images and producing a similarity score between 0 and 1. Therefore the generator returns pairs of image batches x1 and x2 as well as the corresponding batch of labels y. Each label is either 0 (FPs belonging to different individuals) or 1 (FPs belonging to the same individual).\n",
        "\n",
        "Pairs of FP are produced by pairing randomly images from the database. The field **samefreq** is used to balance the dataset as it controls the ratio of same to different individual pairs.\n",
        "\n",
        "I have used the package **imgaug** to augment the images by producing geometric transformations to each image so the dataset has more variability and training makes the network more able to generalise. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGt4o8XmMHWV"
      },
      "source": [
        "class FPData(tensorflow.keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, img_pathlist, augmentation=True, dataset_size=3200, img_channels=3):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_channels = img_channels\n",
        "        self.img_pathlist = img_pathlist\n",
        "        self.seq = iaa.Sequential(\n",
        "            [\n",
        "             iaa.Fliplr(0.5),\n",
        "             iaa.Flipud(0.5),\n",
        "             #iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),\n",
        "             #iaa.ContrastNormalization((0.75, 1.5)),\n",
        "             #iaa.AdditiveGaussianNoise(\n",
        "             #    loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),\n",
        "             #iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "             iaa.Affine(\n",
        "                 scale={\n",
        "                     \"x\": (0.8, 1.2),\n",
        "                     \"y\": (0.8, 1.2)\n",
        "                },\n",
        "                translate_percent={\n",
        "                    \"x\": (-0.1, 0.1),\n",
        "                    \"y\": (-0.1, 0.1)\n",
        "                },\n",
        "                rotate=(-5, 5),\n",
        "                shear=(-8, 8)\n",
        "                )\n",
        "             ],\n",
        "             random_order=True)\n",
        "        self.augmentation = augmentation\n",
        "        # How often identical images are presented to the network\n",
        "        self.samefreq = 0.5\n",
        "        # Load database of images/id labels\n",
        "        DB = self.load_DB()\n",
        "        self.img_DB = DB[0]\n",
        "        self.labels_DB = DB[1]\n",
        "        self.dataset_size = dataset_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ret = []\n",
        "        x1 = np.zeros((self.batch_size,) + self.img_size + (self.img_channels,), dtype=\"float32\")\n",
        "        x2 = np.zeros((self.batch_size,) + self.img_size + (self.img_channels,), dtype=\"float32\")\n",
        "        y = np.zeros((self.batch_size,), dtype=\"uint8\")\n",
        "        img_db = self.img_DB\n",
        "        labels_db = self.labels_DB\n",
        "        for i in range(self.batch_size):\n",
        "            # Pick a random entry in db (img1)\n",
        "            img1_idx = random.randint(0, img_db.shape[0]-1)\n",
        "            if random.random() < self.samefreq:\n",
        "                # Read individual id\n",
        "                id = labels_db[img1_idx]\n",
        "                # Find what entries belong to the same individual\n",
        "                id_args = np.argwhere(labels_db == id)\n",
        "                # set label for same individual\n",
        "                label = 1\n",
        "            else:\n",
        "                # Read individual id\n",
        "                id = labels_db[img1_idx]\n",
        "                # Find what entries belong to a different individual\n",
        "                id_args = np.argwhere(labels_db != id)\n",
        "                # set label for different individual \n",
        "                label = 0\n",
        "            # pick one of the allowed entries at random (img2)\n",
        "            img2_idx = random.sample(id_args.tolist(), 1)\n",
        "            # fetch images\n",
        "            img1 = img_db[img1_idx]\n",
        "            img2 = img_db[img2_idx]\n",
        "            # insert images and label in the batch\n",
        "            x1[i] = img1\n",
        "            x2[i] = img2\n",
        "            y[i] = label\n",
        "            ret.append(([x1[i], x2[i]]))\n",
        "        return [x1, x2], y\n",
        "        #return ret, y\n",
        "\n",
        "    def load_DB(self):\n",
        "        pathlist = self.img_pathlist\n",
        "        img_DB = np.zeros((len(pathlist),) + self.img_size + (self.img_channels,), dtype=np.float32)\n",
        "        labels_DB = np.zeros((len(pathlist),), dtype=np.uint8)\n",
        "        for i, path in enumerate(pathlist):\n",
        "            dir, filename = os.path.split(path)\n",
        "            # Individual ID\n",
        "            id = int(filename[1:3])\n",
        "            # Image\n",
        "            img = load_img(path, color_mode='rgb')\n",
        "            img_array = img_to_array(img)/255.\n",
        "            # Add image/label to database\n",
        "            img_DB[i] = img_array\n",
        "            labels_DB[i] = id\n",
        "        return img_DB, labels_DB"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXHGwrB4esE5"
      },
      "source": [
        "def get_pathlist(dir):\n",
        "    pathlist = []\n",
        "    for root, _, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            path = os.path.join(root, file)\n",
        "            if '.png' not in file or 'checkpoint' in file:\n",
        "                print('Not included in pathlist: ', file)\n",
        "                continue\n",
        "            pathlist.append(path)\n",
        "    return pathlist\n",
        "            \n",
        "\n",
        "\n",
        "train_pathlist = get_pathlist(output_train)\n",
        "val_pathlist = get_pathlist(output_val)\n",
        "\n",
        "random.shuffle(train_pathlist)\n",
        "random.shuffle(val_pathlist)\n",
        "\n",
        "trainsteps_epoch = ntrainfiles//batch_size\n",
        "valsteps_epoch = nvalfiles//batch_size\n",
        "\n",
        "train_gen = FPData(\n",
        "    batch_size,\n",
        "    (img_h,img_w),\n",
        "    train_pathlist,\n",
        "    augmentation=True,\n",
        "    dataset_size=ntrainfiles\n",
        ")\n",
        "\n",
        "val_gen = FPData(\n",
        "    batch_size, \n",
        "    (img_h,img_w), \n",
        "    val_pathlist,\n",
        "    augmentation=False,\n",
        "    dataset_size=nvalfiles)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFktL62EPfke"
      },
      "source": [
        "## Model architecture\n",
        "The concept of choice here has been a siamese network, this is, two identical CNN networks that extract features from two corresponding input images. The networks are joined by the fully-connected head in order to produce a single similarity score for the comparison of the two images. \n",
        "\n",
        "The way I have implemented this is by using a VGG16 network as the CNN base of the architecture. The VGG16 is pretrained on Imagenet so it can exploit the learned features in learning this new task (transfer learning). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMuNYAOIZfBH",
        "outputId": "bd7fcb3b-bf71-4f69-835d-83c2a8bda5cc"
      },
      "source": [
        "def siamese_model():\n",
        "    input_1 = layers.Input(shape=(img_h, img_w, 3))\n",
        "    input_2 = layers.Input(shape=(img_h, img_w, 3))\n",
        "\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_w, img_h, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "\n",
        "    v1 = Flatten()(x1)\n",
        "    v2 = Flatten()(x2)\n",
        "\n",
        "    x = Concatenate(axis=-1)([v1, v2])\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(1e-5))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    plot_model(model, \"./model.png\", show_shapes=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = siamese_model()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "vgg16 (Functional)              (None, 7, 7, 512)    14714688    input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 25088)        0           vgg16[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 25088)        0           vgg16[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 50176)        0           flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          6422656     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            129         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 21,137,473\n",
            "Trainable params: 6,422,785\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "mEPdwE8xkXvr",
        "outputId": "b0478719-0236-44d5-c3de-bc3f1fc2e076"
      },
      "source": [
        "callbacks = [\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "        \"siamese.h5\",\n",
        "        monitor = 'loss',\n",
        "        save_best_only=True,\n",
        "    )]\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "                    trainsteps_epoch,                 \n",
        "                    validation_data=val_gen,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks = callbacks\n",
        "                    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 53s 522ms/step - loss: 0.7146 - acc: 0.4972 - val_loss: 0.6972 - val_acc: 0.4756\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 53s 536ms/step - loss: 0.6911 - acc: 0.5353 - val_loss: 0.6972 - val_acc: 0.5253\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 53s 530ms/step - loss: 0.6674 - acc: 0.5959 - val_loss: 0.7119 - val_acc: 0.5016\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 53s 531ms/step - loss: 0.6448 - acc: 0.6566 - val_loss: 0.6921 - val_acc: 0.5381\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 53s 533ms/step - loss: 0.6227 - acc: 0.7091 - val_loss: 0.6962 - val_acc: 0.4991\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 53s 533ms/step - loss: 0.5941 - acc: 0.7578 - val_loss: 0.7122 - val_acc: 0.5100\n",
            "Epoch 7/30\n",
            " 49/100 [=============>................] - ETA: 13s - loss: 0.5645 - acc: 0.7927"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d3a4f3883b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_aV9OImscOn"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "epoch = [x for x in range(len(loss))]\n",
        "print(epoch)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Siamese network, loss\")\n",
        "plt.plot(epoch, val_loss, label = \"validation loss\")\n",
        "plt.plot(epoch, loss, label = \"train loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.savefig('./figure1')\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Siamese network, accuracy\")\n",
        "plt.plot(epoch, val_acc, label = \"validation accuracy\")\n",
        "plt.plot(epoch, acc, label = \"train accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig('./figure2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54hd5V7xXtBZ"
      },
      "source": [
        "#model = models.load_model('siamese.h5')\n",
        "img_p = load_img('/content/fp_perpetrator.png', color_mode='rgb')\n",
        "img_p_array = img_to_array(img_p)\n",
        "img_p_array = cv2.resize(img_p_array, (img_h,img_w))/255.\n",
        "print(img_p_array.shape)\n",
        "img_q = load_img('/content/DB1_B/107_8.png', color_mode='rgb')\n",
        "img_q_array = img_to_array(img_q)/255.\n",
        "print(img_q_array.shape)\n",
        "print(model.predict([np.array([img_p_array]), np.array([img_q_array])], batch_size=1))\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plt.imshow(img_p_array)\n",
        "plt.subplot(122)\n",
        "plt.imshow(img_q_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RB_YzTir-m"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3ZSWFjgBs6"
      },
      "source": [
        "# Thaw VGG16's top block for fine tuning\n",
        "base_model = model.layers[2]\n",
        "block5_layers = [layer for layer in base_model.layers[-4:-1]]\n",
        "for layer in block5_layers:\n",
        "    layer.trainable = True\n",
        "    print(layer._name, \" -> Trainable\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wXPQYg6i_0F"
      },
      "source": [
        "### Perform fine tuning\n",
        "\n",
        "Fine-tune at a lower learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_GzXTD7jGvy"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(1e-5))\n",
        "\n",
        "callbacks = [\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "        \"siamese_fine-tuned.h5\",\n",
        "        monitor = 'loss',\n",
        "        save_best_only=True,\n",
        "    )]\n",
        "\n",
        "history_fine_tuning = model.fit_generator(train_gen,\n",
        "                    trainsteps_epoch,                 \n",
        "                    validation_data=val_gen,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks = callbacks\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIsGr1YIrDve"
      },
      "source": [
        "loss = history_fine_tuning.history['loss']\n",
        "val_loss = history_fine_tuning.history['val_loss']\n",
        "acc = history_fine_tuning.history['acc']\n",
        "val_acc = history_fine_tuning.history['val_acc']\n",
        "epoch = [x for x in range(len(loss))]\n",
        "print(epoch)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Fine-tuning siamese network, loss\")\n",
        "plt.plot(epoch, val_loss, label = \"validation loss\")\n",
        "plt.plot(epoch, loss, label = \"train loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.savefig('./figure1')\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Fine-tuning siamese network, accuracy\")\n",
        "plt.plot(epoch, val_acc, label = \"validation accuracy\")\n",
        "plt.plot(epoch, acc, label = \"train accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig('./figure2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3dA90JDrXTX"
      },
      "source": [
        "#model = models.load_model('siamese.h5')\n",
        "img_p = load_img('/content/fp_perpetrator.png', color_mode='rgb')\n",
        "img_p_array = img_to_array(img_p)\n",
        "img_p_array = cv2.resize(img_p_array, (img_h,img_w))/255.\n",
        "print(img_p_array.shape)\n",
        "img_q = load_img('/content/DB1_B/104_1.png', color_mode='rgb')\n",
        "img_q_array = img_to_array(img_q)/255.\n",
        "print(img_q_array.shape)\n",
        "print(model.predict([np.array([img_p_array]), np.array([img_q_array])], batch_size=1))\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plt.imshow(img_p_array)\n",
        "plt.subplot(122)\n",
        "plt.imshow(img_q_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-mVSQBZSNQ"
      },
      "source": [
        "#model = models.load_model('siamese_fine-tuned.h5')\n",
        "\n",
        "def similarity_prediction(img1, img2, model):\n",
        "    pred = model.predict([np.array([img1]), np.array([img2])])\n",
        "    pred = pred[0][0]\n",
        "    return pred\n",
        "\n",
        "data_gen = FPData(32, (224,224), train_pathlist)\n",
        "images_db = data_gen.img_DB\n",
        "labels_db = data_gen.labels_DB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIwzDLrzmgAA"
      },
      "source": [
        "img_p = load_img('/content/fp_perpetrator.png', color_mode='rgb')\n",
        "img_p_array = img_to_array(img_p)\n",
        "img_p_array = cv2.resize(img_p_array, (img_h,img_w))/255.\n",
        "\n",
        "data = []\n",
        "for label, image in zip(labels_db, images_db):\n",
        "    data.append([str(label), similarity_prediction(img_p_array, image, model)])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmOgKpDGqHOI"
      },
      "source": [
        "perpetrator_df = pd.DataFrame(data, columns=['label', 'd'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCFtLKNyf8bW"
      },
      "source": [
        "perpetrator_df[0:17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZUX9cxoqrav"
      },
      "source": [
        "# Function to plot mean scores and errors\n",
        "def plot_mean_error(mean_series, error_series, figsize=(8,4)):\n",
        "    mean_series.combine(error_series, lambda x,y: str(x)[:7]+\"+/-\"+str(y)[:7]).sort_values(ascending=False)\n",
        "    plt.figure(figsize=figsize)\n",
        "    ax = mean_series.plot(yerr=error_series)\n",
        "    ax.set_xlabel('Individual')\n",
        "    ax.set_ylabel('Mean score')\n",
        "    ax.set_xticks(range(10))\n",
        "    ax.set_xticklabels(mean_series.sort_values(ascending=False).index)\n",
        "    \n",
        "fp_mean = perpetrator_df.groupby('label').d.mean().sort_values(ascending=False)\n",
        "fp_std = perpetrator_df.groupby('label').d.std()\n",
        "fp_mean.combine(fp_std, lambda x,y: str(x)[:7]+\"+/-\"+str(y)[:7]).sort_values(ascending=False)\n",
        "\n",
        "plot_mean_error(fp_mean, fp_std)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}